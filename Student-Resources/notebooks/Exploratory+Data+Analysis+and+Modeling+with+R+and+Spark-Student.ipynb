{"nbformat_minor": 2, "cells": [{"source": "# Exploratory Data Analysis and Modeling with R and Spark\n\nThis module describes how to do exploratory data analysis with R and Spark using the `sparklyr` package, and Microsoft R Server.", "cell_type": "markdown", "metadata": {}}, {"source": "## Copying Library Over from HDFS", "cell_type": "markdown", "metadata": {}}, {"source": "Before we get started, let's make sure we have the necessary libraries. If you saved your user library from the edge node over to HDFS, we can copy that over to our head node's user library.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ".libPaths()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "list.files(.libPaths()[1])", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "rxHadoopListFiles(\"/Rlib/3.3\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "rxHadoopCopyToLocal(\"/Rlib/3.3/*\", .libPaths()[1])", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "list.files(.libPaths()[1])", "outputs": [], "metadata": {"collapsed": false}}, {"source": "# Create Spark Context\n\nThe `sparklyr` package has a handy function for creating a Spark context. This differs from the method that is used by the `SparkR` package.\n\nThere seems to be some error when loading the namespace of the copied version of `sparklyr`, probably due to some environmental variables. Let's reinstall the package.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "r <- getOption('repos')\n# set mirror to something a bit more recent\nmran_date <- Sys.Date() - 1\nr[[\"CRAN\"]] <- paste0(\"https://mran.revolutionanalytics.com/snapshot/\", mran_date)\noptions(repos = r)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "install.packages(\"sparklyr\")", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "library(sparklyr)\n\n## D13V2, 56 gigs of ram, 8 cores of memory\n## 6/2 = 3 executors with two cores each\n## 54/3 = 18 gigs of ram per executor\n\nconf <- spark_config()\nconf$spark.executor.instances <- 12\nconf$\nconf$'sparklyr.shell.executor-memory' <- \"17g\"\nconf$'sparklyr.shell.driver-memory' <- \"17g\"\nconf$spark.executor.cores <- 2\nconf$spark.executor.memory <- \"17G\"\nconf$spark.yarn.am.cores  <- 2\nconf$spark.yarn.am.memory <- \"1G\"\nconf$spark.dynamicAllocation.enabled <- \"false\"\n\nsc <- spark_connect(master = \"yarn-client\", config = conf)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Create Spark DataFrame", "cell_type": "markdown", "metadata": {}}, {"source": "Check the existing tables in the Hive metastore of our cluster.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "src_tbls(sc)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Create a path to the downloaded taxi dataset.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "wasb_taxi <- \"/NYCTaxi/sample\"\n# rxHadoopListFiles(\"/\")\n# rxHadoopMakeDir(wasb_taxi)\n# rxHadoopCopyFromLocal(\"taxi_large.csv\", wasb_taxi)\n# rxHadoopCommand(\"fs -cat /NYCTaxi/sample/taxi_large.csv | head\")\n", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "system.time(taxi <- spark_read_csv(sc,\n                       path = wasb_taxi,\n                       \"taxisample\",\n                       header = TRUE))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now if we check, we should see the taxi dataset in our Hive metastore.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "src_tbls(sc)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Exploratory Data Analysis\n\n### Feature Engineering -- Add Tip Column\n\nThe `sparklyr` package can be used in conjuction with the `dplyr` package. It basically uses SparkSQL/HiveSQL to convert `dplyr` functions into `SQL` equivalents that can be delivered to Spark SQL and the Catalyst optimizer. The reason this works is that SQL DataFrames created by the `sparklyr` package all inherit the *tbl_sql* class.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "library(dplyr)\nclass(taxi)\nsample_taxi <- mutate(taxi, tip_pct = tip_amount/fare_amount)\nexplain(sample_taxi)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "\nSame exact syntax as before, but this time, all computation takes place in Spark.\n\n\n### Create summary functions\n\nJust as before, we can create aggregation functions. Here's the same function from before, that worked with `data.frames` and with `xdfs` using the `dplyrXdf` package.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\ntaxi_hood_sum <- function(taxi_data = taxi_df, ...) {\n  \n  load(url(\"http://alizaidi.blob.core.windows.net/training/manhattan.RData\"))\n  \n  taxi_data %>% \n    filter(pickup_nhood %in% manhattan_hoods,\n           dropoff_nhood %in% manhattan_hoods, ...) %>% \n    group_by(dropoff_nhood, pickup_nhood) %>% \n    summarize(ave_tip = mean(tip_pct), \n              ave_dist = mean(trip_distance)) %>% \n    filter(ave_dist > 3, ave_tip > 0.05) -> sum_df\n  \n  return(sum_df)\n  \n}\n", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Visualize Summarized Data\n\nWe can visualize our summarized data above using our favorite R visualization tools. In this case, our summarized dataset is small enough to fit in-memory of our local R driver. We'll use the `collect` function to convert the Spark DataFrame into an R `data.frame`. Let's check and make sure our data is small enough to fit comfortably in-memory of our local context.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "library(dplyr)\ncount(ungroup(taxi_hood_sum(sample_taxi)))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We'll use a wrapper function to create our plot, taking in the collected R data.frame and visualizing it using `ggplot2`.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\ntile_plot_hood <- function(df = taxi_hood_sum()) {\n  \n  library(ggplot2)\n  \n  ggplot(data = df, aes(x = pickup_nhood, y = dropoff_nhood)) + \n    geom_tile(aes(fill = ave_tip), colour = \"white\") + \n    theme_bw() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1),\n          legend.position = 'bottom') +\n    scale_fill_gradient(low = \"white\", high = \"steelblue\") -> gplot\n  \n  return(gplot)\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Calculate Summary, Collect Results, Plot... Profit\n\nWe can chain together our results. We can use the `collect` function to bring the aggregated/summarized dataset into memory, so `ggplot2` can understand it.\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "taxi_summary <- taxi_hood_sum(sample_taxi)\nclass(taxi_summary)\n\ntaxi_df <- taxi_summary %>% collect\n\ntaxi_df\ng <- tile_plot_hood(taxi_df)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "g", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "IRdisplay::display_html(repr::repr_html(ggplotly(g)))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Machine Learning Pipelines with Spark \n\nSince Spark 1.3, there has been much interest in creating a simple, interactive machine learning pipeline that represents a full data science application from start to end. The `mllib` package was created to address this need.\n\nLooks very similar to the python `scikit-learn` package. For more of the available functionality, refer to [sparklyr-ml](http://spark.rstudio.com/mllib.html) website.\n\n\n### Create Training and Split\n\nIn addition to modeling functions, there are numerous `mllib` functions for pre-processing and transformations. Here's well use the partition function to split our data into training and test sets.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "partitions <- sample_taxi %>%\n  sdf_partition(training = 0.75, test = 0.25, seed = 1099)\n\nstr(partitions)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Fit a Linear Regression Model\n\nWe'll now train a simple linear regression model (one feature variable) on our training dataset.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\nsystem.time(\n    fit <- partitions$training %>% \n      filter(tip_pct < 0.5) %>% \n      ml_linear_regression(response = \"tip_pct\", features = c(\"trip_distance\"))\n)\nfit", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Plot Results\n\nThis may not work for larger datasets, so we'll sample, and for our current example we can simply select the two columns we used in our model, and collect them into local memory for visualization.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\nlibrary(ggplot2)\n\npartitions$test %>%\n  select(tip_pct, trip_distance) %>%\n  filter(tip_pct < 0.5) %>% \n  sample_n(10^5) %>% \n  collect %>%\n  ggplot(aes(trip_distance, tip_pct)) +\n  geom_point(size = 2, alpha = 0.5) +\n  geom_abline(aes(slope = coef(fit)[[\"trip_distance\"]],\n                  intercept = coef(fit)[[\"(Intercept)\"]]),\n              color = \"red\") +\n  scale_y_continuous(label = scales::percent) +\n  labs(\n    x = \"Trip Distance in Miles\",\n    y = \"Tip Percentage (%)\",\n    title = \"Linear Regression: Tip Percent ~ Trip Distance\",\n    subtitle = \"Spark.ML linear regression to predict tip percentage.\"\n  )\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Classification Tree\n\nTry out a binary classification model using the ensemble tree algoritms. \n\nLet's first create a binary column to use as our response variable:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\ntaxi_binary <- sample_taxi %>%\n  ft_binarizer(input_col = \"tip_pct\",\n               output_col = \"good_tip\",\n               threshold = 0.1)\n\npartitions <- taxi_binary %>%\n  sdf_partition(training = 0.75, test = 0.25, seed = 1099)\n", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Train a single decision tree.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\nsystem.time(\n    fit_dtree <- partitions$training %>% \n      ml_decision_tree(response = \"good_tip\", \n                       features = c(\"payment_type\", \"passenger_count\", \"trip_distance\"), \n                       type = \"classification\")\n\n)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let's try predicting with the fitted decision tree model:\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "score_dtree <- sdf_predict(fit_dtree, partitions$test)\n", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "ml_tree_feature_importance(sc, fit_dtree)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "ml_tree_feature_importance(sc, fit_dtree) %>%\n    mutate(importance = as.numeric(as.character(importance))) %>%\n    ggplot(aes(x = feature, y = importance)) +\n        geom_bar(stat = 'identity') +\n        theme_minimal() +\n        coord_flip()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "That was for a single tree. Let's try to train a ensemble tree using the random forest function:\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "system.time(\n    fit_dforest <- partitions$training %>% \n      ml_random_forest(response = \"good_tip\", \n                       features = c(\"payment_type\", \"passenger_count\", \"trip_distance\"), \n                       type = \"classification\"))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "score_dforest <- sdf_predict(fit_dforest, partitions$test)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Create a Confusion Matrix\n\nNow that we have our predicted results, we could create a confusion matrix of our predictions vs the actuals.\n\nIf we want to work entirely with Spark DataFrames, we can use the `sdf_predict` function and then group_by the predictions and actual values:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "sdf_conf_df <- function(predictions = score_dtree) {\n  \n  \n  conf_sdf <- predictions %>% group_by(prediction, good_tip) %>% tally()\n  \n  return(conf_sdf)\n  \n}\n\nsdf_conf_df()", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "library(tidyr)\n\nconf_spread <- . %>% spread(key = \"good_tip\", value  = \"n\")\n\ndtree_conf <- sdf_conf_df() %>% collect %>% conf_spread\n\n# create ratios for confusion matrix\ndtree_conf <- dtree_conf[, 2:3]\ndtree_conf/sum(dtree_conf)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "\ndtree_auc <- ml_binary_classification_eval(predicted_tbl_spark = score_dtree, \n                                            label = \"good_tip\", score = \"probability\")\n\ndforest_auc <- ml_binary_classification_eval(predicted_tbl_spark = score_dforest, \n                                             label = \"good_tip\", score = \"probability\")\n\n\n### metric choices: c(\"f1\", \"precision\", \"recall\", \"weightedPrecision\", \"weightedRecall\", \"accuracy\")\n\ndforest_accuracy <- ml_classification_eval(predicted_tbl_spark = score_dforest, \n                                           label = \"good_tip\", \n                                           predicted_lbl = \"prediction\", \n                                           metric = \"accuracy\")\n\ndtree_auc\ndforest_auc\ndforest_accuracy\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Let's see how the feature variables effected our predictions.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "feature_importance <- ml_tree_feature_importance(sc, fit_dforest) %>%\n    mutate(importance = as.numeric(levels(importance))[importance]) %>%\n    mutate(feature = as.character(feature))", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "feature_importance %>%\n    mutate(importance = as.numeric(as.character(importance))) %>%\n    ggplot(aes(x = feature, y = importance)) +\n        geom_bar(stat = 'identity') +\n        theme_minimal() +\n        coord_flip()", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Modeling with RxSpark\n\nThis section shows how to use the `RxSpark` compute context for modeling.\n\n# Locate RevoShare dir\n\nEvery MRS installation on a HDFS environment creates a share directory on HDFS. By default, each user will have her own shared directory under the `/user/RevoShare/` file path.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\nrxHadoopListFiles(\"/user/RevoShare/\")\nusername <- \"sshuser\"\ndata_path <- file.path(\"/user/RevoShare\", username)", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "rxHadoopListFiles(\"/user/RevoShare/sshuser/taxiTextXdf\")", "outputs": [], "metadata": {"collapsed": false}}, {"source": "# Saving the Spark DataFrame to CSV\n\nThe `RxSpark` and the Spark Compute contexts are completely distinct compute environments. In order to use the `rx` functions, we need to move the Spark DataFrame into a format that MRS can understand.\n", "cell_type": "markdown", "metadata": {}}, {"source": "### Write Sample Taxi to RevoShare ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "library(sparklyr)\n\nspark_write_csv(taxi_binary, \n                path = file.path(data_path, 'sampleTaxi'))\n\n# spark_write_parquet(taxi_binary,\n#                     path = file.path(data_path, \"taxiParquet\"))\n", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Since we've saved our Spark DataFrame from the `sparklyr` application to persistent storage, we can shut down our `sparklyr` connection.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "spark_disconnect(sc)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Writing to HDFS adds a directory giving a flag for the completion rate of our write job. Whenever it completes successfully, we'd like to delete the _SUCCESS_ flag.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\nrxHadoopListFiles(file.path(data_path, \"sampleTaxi\"))\nfile_to_delete <- file.path(data_path, \n                            \"sampleTaxi\", \"_SUCCESS\")\ndelete_command <- paste(\"fs -rm\", file_to_delete)\nrxHadoopCommand(delete_command)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "## Create HDFS and Spark Contexts for Revo\n\nLet's create the pointers to the file paths and HDFS to use the `RxSpark` compute context. We'll need a pointer to our `inData` object, which is the CSV file in the `sampleTaxi` directory, as well as a pointer to an XDF file we will save our results to (the `outFile` argument).", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\nmyNameNode <- \"default\"\nmyPort <- 0\nhdfsFS <- RxHdfsFileSystem()\n\ntaxi_text <- RxTextData(file.path(data_path,\n                                  \"sampleTaxi\"),\n                        fileSystem = hdfsFS)\n\n# taxi_parquet <- RxParquetData(file.path(data_path, \"taxiParquet\"),\n#                               fileSystem = hdfsFS)\n\ntaxi_xdf <- RxXdfData(file.path(data_path, \"taxiTextXdf\"),\n                      fileSystem = hdfsFS)\n", "outputs": [], "metadata": {"collapsed": true}}, {"source": "### Create RxSpark Compute Context", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "computeContext <- RxSpark(consoleOutput=TRUE,\n                          nameNode=myNameNode,\n                          port=myPort, \n                          numExecutors = 12,\n                          executorCores=2, \n                          executorMem = \"12g\", \n                          executorOverheadMem = \"5g\", \n                          persistentRun = TRUE, \n                          extraSparkConfig = \"--conf spark.speculation=true\")\n\nrxSetComputeContext(computeContext)", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Import CSV file into XDF File. ", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "col_classes <- c('VendorID' = \"factor\",\n                 'passenger_count' = \"integer\",\n                 'trip_distance' = \"numeric\",\n                 'RateCodeID' = \"factor\",\n                 'store_and_fwd_flag' = \"factor\",\n                 'payment_type' = \"factor\",\n                 'fare_amount' = \"numeric\",\n                 'tip_amount' = \"numeric\",\n                 'tolls_amount' = \"numeric\",\n                 'pickup_hour' = \"factor\",\n                 'pickup_dow' = \"factor\", \n                 'dropoff_hour' = \"factor\",\n                 'dropoff_dow' = \"factor\",\n                 'pickup_nhood' = \"factor\",\n                 'dropoff_nhood' = \"factor\",\n                 'kSplits' = \"factor\",\n                 'tip_pct' = \"numeric\",\n                 'good_tip' = \"factor\")\n\nsystem.time(\n    rxImport(inData = taxi_text, \n         taxi_xdf, \n         overwrite = TRUE, colClasses = col_classes)\n\n)", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "rxGetInfo(taxi_xdf, getVarInfo = TRUE)", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Visualizations with MRS\n\nLet's use MRS to summarize our data. We'll use the `rxCube` function to create a tabulation of average tips for each combination of pickup day of week and pickup hour. This gives us a sense of the temporal distribution of the tip percent variable.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "\ntip_dist_df <- rxCube(tip_pct ~ pickup_hour + pickup_dow, \n                      data = taxi_xdf, returnDataFrame = TRUE)\n\nlibrary(ggplot2)\nlibrary(magrittr)\n\ntip_dist_df %>% ggplot(aes(x = pickup_hour, y = pickup_dow, fill = tip_pct)) +\n  geom_tile() + theme_minimal() + \n  scale_fill_continuous(label = scales::percent) +\n  labs(x = \"Pickup Hour\", y = \"Pickup Day of Week\", fill = \"Tip Percent\",\n      title = \"Distribution of Tip Percents\",\n      subtitle = \"Do Passengers Tip More in the AM?\")\n", "outputs": [], "metadata": {"collapsed": false}}, {"source": "## Modeling with MRS\n\n### Creating Linear Models\n\nLet's predict tip_pct as a function of distance and neighborhoods. In order to ensure that the neighbhorhood columns are treated as categorical, we need them to factors. `RevoScaleR` and the `RxSpark` compute context are more picky about factor types than base R models, since they utilize data that is chunked and stored in distributed file systems. \n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "system.time(linmod <- rxLinMod(tip_pct ~ pickup_nhood + pickup_hour + trip_distance, \n                               data = taxi_xdf, \n                               cube = TRUE))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "The `cube` argument parallelizes training over the levels of the categorical feature variables. In our case, this means we'll train, in parallel, the coefficients for the various pickup neighborhoods and pickup hour combinations.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "system.time(dtree <- rxDTree(good_tip ~ pickup_nhood + pickup_hour + trip_distance, \n                               data = taxi_xdf, minSplit = 10))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "library(RevoTreeView)\nplot(createTreeView(dtree))\nIRdisplay::display_html((plot(createTreeView(dtree))))", "outputs": [], "metadata": {"collapsed": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "R", "name": "ir", "language": "R"}, "language_info": {"mimetype": "text/x-r-source", "version": "3.3.2", "name": "R", "pygments_lexer": "r", "file_extension": ".r", "codemirror_mode": "r"}}}